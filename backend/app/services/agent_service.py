from typing import Awaitable, Callable, Optional, Type, Any, Dict, AsyncGenerator, Union
from fastapi import Response
from json_repair import repair_json
from pydantic import BaseModel, ValidationError
from pydantic_ai import Agent, ModelResponse, ModelRetry
from pydantic_ai.models.openai import OpenAIChatModel
from pydantic_ai.providers.openai import OpenAIProvider
from pydantic_ai.models.anthropic import AnthropicModel
from pydantic_ai.providers.anthropic import AnthropicProvider
from pydantic_ai.models.google import GoogleModel
from pydantic_ai.providers.google import GoogleProvider
from pydantic_ai.settings import ModelSettings
from sqlmodel import Session
from app.services import llm_config_service
from loguru import logger
from app.schemas.ai import ContinuationRequest, AssistantChatRequest
from app.services import prompt_service
from app.db.models import LLMConfig
import asyncio
import json
import re
import os
from datetime import datetime

# å¯¼å…¥éœ€è¦æ ¡éªŒçš„æ¨¡å‹
from app.schemas.wizard import StageLine, ChapterOutline, Chapter
import re

# Read max tool call retries from env, default 3
MAX_TOOL_CALL_RETRIES = int(os.getenv('MAX_TOOL_CALL_RETRIES', '3'))

_TOKEN_REGEX = re.compile(
    r"""
    ([A-Za-z]+)               # English word (consecutive letters count as 1)
    |([0-9])                 # 1 digit counts as 1
    |([\u4E00-\u9FFF])       # Single Chinese char counts as 1
    |(\S)                     # Other non-whitespace symbol/punctuation counts as 1
    """,
    re.VERBOSE,
)

def _estimate_tokens(text: str) -> int:
    """Estimate tokens by rule:
    - 1 Chinese char = 1
    - 1 English word = 1
    - 1 digit = 1
    - 1 symbol = 1
    Whitespace ignored.
    """
    if not text:
        return 0
    try:
        return sum(1 for _ in _TOKEN_REGEX.finditer(text))
    except Exception:
        # Fallback: Count non-whitespace chars
        return sum(1 for ch in (text or "") if not ch.isspace())

from app.services import llm_config_service as _llm_svc

def _calc_input_tokens(system_prompt: Optional[str], user_prompt: Optional[str]) -> int:
    sys_part = system_prompt or ""
    usr_part = user_prompt or ""
    return _estimate_tokens(sys_part+usr_part) 


def _precheck_quota(session: Session, llm_config_id: int, input_tokens: int, need_calls: int = 1) -> None:
    ok, reason = _llm_svc.can_consume(session, llm_config_id, input_tokens, 0, need_calls)
    return ok,reason


def _record_usage(session: Session, llm_config_id: int, input_tokens: int, output_tokens: int, calls: int = 1, aborted: bool = False) -> None:
    try:
        _llm_svc.accumulate_usage(session, llm_config_id, max(0, input_tokens), max(0, output_tokens), max(0, calls), aborted=aborted)
    except Exception as stat_e:
        logger.warning(f"è®°å½• LLM ç»Ÿè®¡å¤±è´¥: {stat_e}")

def _get_agent(
    session: Session,
    llm_config_id: int,
    output_type: Optional[Type[BaseModel]] = None,
    system_prompt: str = '',
    temperature: float = 0.7,
    max_tokens: Optional[int] = None,
    timeout: float = 64,
    deps_type: Type = str,
    tools: list = None,) -> Agent:
    """
    Get a configured LLM Agent instance based on LLM config and expected output type.
    Unified use of ModelSettings to set temperature/max_tokens/timeout.
    
    - deps_type: Dependency injection type (default str)
    - tools: Tool list (Pydantic AI Tool objects)
    - output_type: Output type (None means allow text and tool calls)
    """
    llm_config = llm_config_service.get_llm_config(session, llm_config_id)
    if not llm_config:
        raise ValueError(f"LLM Config not found, ID: {llm_config_id}")

    if not llm_config.api_key:
        raise ValueError(f"API Key not found for LLM Config {llm_config.display_name or llm_config.model_name}")
    print(f"=======llm_config.provider:{llm_config.provider}=========")
    # Provider & Model Creation (No longer set temperature/timeout here)
    if llm_config.provider == "openai":
        provider_config = {"api_key": llm_config.api_key}
        if llm_config.api_base:
            provider_config["base_url"] = llm_config.api_base
        provider = OpenAIProvider(**provider_config)
        model = OpenAIChatModel(llm_config.model_name, provider=provider)
    elif llm_config.provider == "anthropic":
        provider_config = {"api_key": llm_config.api_key}
        if llm_config.api_base:
            provider_config["base_url"] = llm_config.api_base
        provider = AnthropicProvider(**provider_config)
        model = AnthropicModel(llm_config.model_name, provider=provider)
    elif llm_config.provider == "google":

        provider = GoogleProvider(api_key=llm_config.api_key)
        model = GoogleModel(llm_config.model_name, provider=provider)
    elif llm_config.provider == "custom":
        provider_config = {"api_key": llm_config.api_key}
        if llm_config.api_base:
            provider_config["base_url"] = llm_config.api_base
        provider = OpenAIProvider(**provider_config)
        model = OpenAIChatModel(llm_config.model_name, provider=provider)
    else:
        raise ValueError(f"Unsupported provider type: {llm_config.provider}")

    # Unified model settings
    settings = ModelSettings(
        temperature=temperature,
        max_tokens=max_tokens,
        timeout=timeout,
        extra_body=None,
    )

    # Create Agent (Supports tools and custom dependency types)
    if output_type is None:
        # If output_type not specified, default allow text output and tool calls
        agent = Agent(
            model, 
            system_prompt=system_prompt, 
            model_settings=settings,
            deps_type=deps_type,
            tools=tools or []
        )
    else:
        # If output_type specified, use Union[output_type, str] to allow text fallback
        agent = Agent(
            model, 
            system_prompt=system_prompt, 
            model_settings=settings,
            output_type=Union[output_type, str],
            deps_type=deps_type,
            tools=tools or []
        )
        agent.output_validator(create_validator(output_type))
    
    return agent

async def run_agent_with_streaming(agent: Agent, *args, **kwargs):
    """
    Iterate to get each streaming response block content using agent.iter() and node.stream(),
    then return the final complete result. Avoid generation failure due to network fluctuation when returning result directly
    """
    async with agent.run_stream(*args, **kwargs) as stream:
        return await stream.get_output()


def check_tool_call(parts: list,is_in_retry_state: bool) -> bool:
    need_tool_call=is_in_retry_state
    include_tool_call=False
    for part in parts:
        if part.part_kind=="text":
            if re.search(r"<notify>[\w\-]+</notify>", part.content):
                need_tool_call=True
    if need_tool_call:
        for part in parts:
            if part.part_kind=="tool-call":
                include_tool_call=True
                break
    return (not need_tool_call) or include_tool_call

from pydantic_ai import (
        FinalResultEvent,
        FunctionToolCallEvent,
        FunctionToolResultEvent,
    )
from pydantic_ai.messages import (
    ModelRequest,
    RetryPromptPart,
    ToolCallPart
)

from pydantic_ai import _agent_graph


async def execute_react_tool(
    tool_name: str,
    tool_args: Dict[str, Any],
    deps: Any,
    tools_map: Dict[str, Callable]) -> Dict[str, Any]:
    """
    Execute tool call in ReAct mode
    
    Args:
        tool_name: Tool name
        tool_args: Tool arguments
        deps: Dependency context
        tools_map: Tool function mapping
        
    Returns:
        Tool execution result {"tool_name": str, "args": dict, "result": Any, "success": bool, "error": Optional[str]}
    """
    logger.info(f"ğŸ”§ [ReAct] Executing tool: {tool_name}")
    logger.info(f"   Arguments: {json.dumps(tool_args, ensure_ascii=False)[:200]}...")
    
    # Verify tool name
    if tool_name not in tools_map:
        error_msg = f"Unknown tool: {tool_name}"
        logger.error(f"âŒ [ReAct] {error_msg}")
        return {
            "tool_name": tool_name,
            "args": tool_args,
            "success": False,
            "error": error_msg
        }
    
    try:
        tool_func = tools_map[tool_name]
        
        # Create simple context object (Compatible with Pydantic AI tool signature)
        class SimpleContext:
            def __init__(self, deps):
                self.deps = deps
        
        ctx = SimpleContext(deps=deps)
        
        # Call tool (Check if ctx parameter is needed)
        import inspect
        sig = inspect.signature(tool_func)
        if 'ctx' in sig.parameters:
            result = tool_func(ctx, **tool_args)
        else:
            result = tool_func(**tool_args)
        
        logger.info(f"âœ… [ReAct] Tool executed successfully: {tool_name}")
        
        return {
            "tool_name": tool_name,
            "args": tool_args,
            "result": result,
            "success": True
        }
    
    except Exception as e:
        error_msg = f"Tool execution failed: {str(e)}"
        logger.error(f"âŒ [ReAct] {error_msg}", exc_info=True)
        return {
            "tool_name": tool_name,
            "args": tool_args,
            "success": False,
            "error": error_msg
        }


async def process_react_text(
    text: str,
    react_accumulated_text: str,
    react_processed_calls: list,
    tool_calls_info: list,
    deps: Any,
    react_tools_map: Dict[str, Callable]) -> AsyncGenerator[Union[str, tuple], None]:
    """
    Process ReAct mode text: Detect tool calls, execute tools, output text
    
    Args:
        text: Current text block
        react_accumulated_text: Accumulated text
        react_processed_calls: List of processed tool call positions
        tool_calls_info: List of tool call info
        deps: Dependency context
        react_tools_map: Tool function mapping
        
    Yields:
        - str: Protocol markers and text content
        - tuple: Last yield returns (updated_accumulated_text, new_tool_count) tuple
    """
    # Accumulate text
    react_accumulated_text += text
    new_tool_count = 0
    
    # Detect and process tool calls
    tool_call_pattern = re.compile(r'<tool_call>(.*?)</tool_call>', re.DOTALL)
    
    for match in tool_call_pattern.finditer(react_accumulated_text):
        match_key = (match.start(), match.end())
        
        # Avoid duplicate processing
        if match_key in react_processed_calls:
            continue
        
        react_processed_calls.append(match_key)
        logger.info(f"[ReAct] Tool call detected (Position {match.start()}-{match.end()})")
        
        # Notify frontend tool call start
        yield "\n\n__TOOL_CALL_DETECTED__\n\n"
        
        # Parse and execute tool
        try:
            tool_json = match.group(1).strip()
            try:
                tool_call_data = json.loads(tool_json)
            except json.JSONDecodeError:
                logger.warning(f"[ReAct] JSON parse failed, attempting auto-repair...")
                repaired_json = repair_json(tool_json)
                tool_call_data = json.loads(repaired_json)
                logger.info(f"[ReAct] JSON repair success")
            
            tool_name = tool_call_data.get("name")
            tool_args = tool_call_data.get("args", {})
            
            # Execute tool
            tool_result = await execute_react_tool(
                tool_name=tool_name,
                tool_args=tool_args,
                deps=deps,
                tools_map=react_tools_map
            )
            
            # Notify frontend tool execution complete
            yield f"__TOOL_EXECUTED__:{json.dumps(tool_result, ensure_ascii=False)}\n\n"
            
            # Record tool call
            tool_calls_info.append(tool_result)
            new_tool_count += 1
            
        except Exception as e:
            error_msg = f"Tool call processing failed: {str(e)}"
            logger.error(f"[ReAct] {error_msg}", exc_info=True)
            yield f"\n\nâŒ {error_msg}\n\n"
    
    # Output text (Frontend will filter out <tool_call> tags)
    yield text
    
    # Finally yield updated accumulated text and new tool count
    yield (react_accumulated_text, new_tool_count)


async def stream_agent_response(
    agent: Agent,
    user_prompt: str,
    *,
    deps=None,
    message_history: list = None,
    track_tool_calls: bool = True,
    max_tool_call_retries: int = None,
    use_react_mode: bool = False,
    react_tools_map: Optional[Dict[str, Callable]] = None) -> AsyncGenerator[str, None]:
    """
    é€šç”¨çš„æµå¼ Agent å“åº”ç”Ÿæˆå™¨ï¼Œæ”¯æŒå·¥å…·è°ƒç”¨å’Œæ–‡æœ¬æµå¼è¾“å‡ºã€‚
    
    **åŸºäº Pydantic AI å®˜æ–¹æ–‡æ¡£å®ç°**ï¼š
    - è‡ªåŠ¨è¿­ä»£ï¼šhttps://ai.pydantic.dev/agents/#iterating-over-an-agents-graph
    - æ‰‹åŠ¨è¿­ä»£ï¼šhttps://ai.pydantic.dev/agents/#using-next-manually
    - å›¾èŠ‚ç‚¹ï¼šhttps://ai.pydantic.dev/graph/
    
    å·¥ä½œåŸç†ï¼š
    1. ä½¿ç”¨æ‰‹åŠ¨è¿­ä»£æ¨¡å¼ run.next() é€ä¸ªå¤„ç†èŠ‚ç‚¹
    2. å¯¹äº ModelRequestNodeï¼š
       - æ£€æµ‹ FinalResultEvent åæµå¼è¾“å‡ºæ–‡æœ¬
       - **ReAct æ¨¡å¼**ï¼šå®æ—¶æ£€æµ‹ <tool_call>...</tool_call> å¹¶æ‰‹åŠ¨æ‰§è¡Œå·¥å…·
    3. å¯¹äº CallToolsNodeï¼ˆä»…æ ‡å‡†æ¨¡å¼ï¼‰ï¼š
       - æ£€æµ‹æ¨¡å‹æ˜¯å¦æ­£ç¡®è°ƒç”¨äº†å·¥å…·ï¼ˆé€šè¿‡ check_tool_callï¼‰
       - å¦‚æœæœªæ­£ç¡®è°ƒç”¨ï¼Œæ³¨å…¥é‡è¯•æç¤ºå¹¶è·³è¿‡èŠ‚ç‚¹ï¼Œæœ€å¤šé‡è¯• max_tool_call_retries æ¬¡
       - å¦‚æœæ­£ç¡®è°ƒç”¨ï¼Œç›‘å¬å·¥å…·æ‰§è¡Œäº‹ä»¶å¹¶æµå¼æ¨é€çŠ¶æ€
    4. æµç»“æŸåè¿”å›å·¥å…·è°ƒç”¨æ‘˜è¦
    5. **ReAct æ¨¡å¼**ï¼šå·¥å…·æ‰§è¡Œåæ³¨å…¥ç»“æœèŠ‚ç‚¹ï¼Œç»§ç»­è¿­ä»£è®© agent åŸºäºç»“æœç”Ÿæˆ
    
    Args:
        agent: Pydantic AI Agent å®ä¾‹ï¼ˆæ ‡å‡†æ¨¡å¼éœ€ç»‘å®šå·¥å…·ï¼ŒReAct æ¨¡å¼æ— éœ€ç»‘å®šï¼‰
        user_prompt: ç”¨æˆ·è¾“å…¥çš„æç¤ºè¯
        deps: ä¾èµ–æ³¨å…¥çš„ä¸Šä¸‹æ–‡å¯¹è±¡
        message_history: æ¶ˆæ¯å†å²
        track_tool_calls: æ˜¯å¦åœ¨æµç»“æŸåè¿”å›å·¥å…·è°ƒç”¨æ‘˜è¦
        max_tool_call_retries: å·¥å…·è°ƒç”¨å¤±è´¥æ—¶çš„æœ€å¤§é‡è¯•æ¬¡æ•°
        use_react_mode: æ˜¯å¦ä½¿ç”¨ ReAct æ¨¡å¼ï¼ˆæ–‡æœ¬æ ¼å¼å·¥å…·è°ƒç”¨ï¼‰
        react_tools_map: ReAct æ¨¡å¼çš„å·¥å…·å‡½æ•°æ˜ å°„è¡¨
        
    Yields:
        å¢é‡æ–‡æœ¬å†…å®¹æˆ–å·¥å…·è°ƒç”¨æ‘˜è¦ï¼ˆJSON æ ¼å¼ï¼‰
    """
    
    
    run_kwargs = {"message_history": message_history} if message_history else {}
    
    # ä½¿ç”¨å…¨å±€é…ç½®çš„æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆå¦‚æœæœªä¼ å…¥ï¼‰
    if max_tool_call_retries is None:
        max_tool_call_retries = MAX_TOOL_CALL_RETRIES
    
    tool_calls_info = []
    tool_call_retry_count = 0  # å·¥å…·è°ƒç”¨é‡è¯•è®¡æ•°å™¨
    
    is_in_retry_state = False
    
    # ReAct æ¨¡å¼ç›¸å…³å˜é‡
    react_accumulated_text = ""  # ReAct æ¨¡å¼ç´¯ç§¯çš„æ–‡æœ¬
    react_processed_calls = []  # å·²å¤„ç†çš„å·¥å…·è°ƒç”¨ï¼ˆå­˜å‚¨ä½ç½®ï¼‰
    react_last_tool_count = 0  # ä¸Šä¸€è½®çš„å·¥å…·è°ƒç”¨æ€»æ•°ï¼ˆç”¨äºè®¡ç®—æœ¬è½®æ–°å¢ï¼‰
    
    # ä½¿ç”¨æ‰‹åŠ¨è¿­ä»£æ¨¡å¼ï¼ˆåŸºäºå®˜æ–¹æ–‡æ¡£ï¼‰
    async with agent.iter(user_prompt, deps=deps, **run_kwargs) as run:
        node = run.next_node
        while True:
            # æ‰‹åŠ¨è·å–ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
            node = await run.next(node)
            
            # è¿­ä»£ç»“æŸ
            if node is None:
                break
            
            logger.info(f"ğŸ“ [stream_agent_response] å½“å‰èŠ‚ç‚¹: {type(node).__name__}")
            
            # ModelRequestNode - æ¨¡å‹è¯·æ±‚èŠ‚ç‚¹ï¼Œå¯èƒ½åŒ…å«æµå¼æ–‡æœ¬è¾“å‡º
            if Agent.is_model_request_node(node):
                async with node.stream(run.ctx) as request_stream:
                    final_result_found = False
                    async for event in request_stream:
                        # æ£€æµ‹åˆ°æœ€ç»ˆç»“æœå¼€å§‹
                        if isinstance(event, FinalResultEvent):
                            final_result_found = True
                            break
                    
                    # å¦‚æœæ£€æµ‹åˆ°æœ€ç»ˆç»“æœï¼Œæµå¼è¾“å‡ºæ–‡æœ¬ï¼ˆå¢é‡æ¨¡å¼ï¼‰
                    if final_result_found:
                        # ReAct æ¨¡å¼ï¼šç´¯ç§¯æ–‡æœ¬å¹¶å®æ—¶æ£€æµ‹å·¥å…·è°ƒç”¨
                        if use_react_mode and react_tools_map:
                            async for output in request_stream.stream_text(delta=True):
                                # ä½¿ç”¨ç»Ÿä¸€çš„ process_react_text å‡½æ•°å¤„ç†æ–‡æœ¬
                                async for chunk in process_react_text(
                                    text=output,
                                    react_accumulated_text=react_accumulated_text,
                                    react_processed_calls=react_processed_calls,
                                    tool_calls_info=tool_calls_info,
                                    deps=deps,
                                    react_tools_map=react_tools_map
                                ):
                                    # æœ€åä¸€ä¸ªè¿”å›å€¼æ˜¯ tupleï¼ŒåŒ…å«æ›´æ–°åçš„ç´¯ç§¯æ–‡æœ¬
                                    if isinstance(chunk, tuple):
                                        react_accumulated_text, _ = chunk
                                    else:
                                        yield chunk
                        
                        # æ ‡å‡†æ¨¡å¼ï¼šç›´æ¥æµå¼è¾“å‡º
                        else:
                            async for output in request_stream.stream_text(delta=True):
                                yield output
                
                # ReAct æ¨¡å¼ï¼šå¦‚æœæœ‰æ–°çš„å·¥å…·è°ƒç”¨ï¼Œæ³¨å…¥å·¥å…·ç»“æœèŠ‚ç‚¹å¹¶ç»§ç»­è¿­ä»£
                if use_react_mode and react_processed_calls and final_result_found:
                    # è®¡ç®—æœ¬è½®æ–°å¢çš„å·¥å…·è°ƒç”¨æ•°é‡ï¼ˆå½“å‰æ€»æ•° - ä¸Šä¸€è½®æ€»æ•°ï¼‰
                    current_tool_count = len(tool_calls_info)
                    new_tools_count = current_tool_count - react_last_tool_count
                    
                    if new_tools_count > 0:
                        logger.info(f"[ReAct] æœ¬è½®æ–°å¢ {new_tools_count} ä¸ªå·¥å…·è°ƒç”¨ï¼Œæ³¨å…¥ç»“æœèŠ‚ç‚¹")
                        
                        # æ„å»ºå·¥å…·ç»“æœæ‘˜è¦æ–‡æœ¬
                        tool_results_text = "\n\n**å·¥å…·æ‰§è¡Œç»“æœ**ï¼š\n\n"
                        for tool_info in tool_calls_info[-new_tools_count:]:
                            if tool_info.get('success'):
                                tool_results_text += f"âœ… {tool_info['tool_name']}\n"
                                tool_results_text += f"```json\n{json.dumps(tool_info['result'], ensure_ascii=False, indent=2)}\n```\n\n"
                            else:
                                tool_results_text += f"âŒ {tool_info['tool_name']}: {tool_info.get('error', 'æœªçŸ¥é”™è¯¯')}\n\n"
                        
                        tool_results_text += "è¯·åŸºäºä»¥ä¸Šå·¥å…·è°ƒç”¨ç»“æœç»§ç»­å›ç­”ç”¨æˆ·ã€‚\n"
                        
                        logger.info(f"[ReAct] å·¥å…·ç»“æœæ‘˜è¦:\n{tool_results_text[:300]}...")
                        
                        # åˆ›å»ºæ–°çš„è¯·æ±‚èŠ‚ç‚¹ï¼ˆæ³¨å…¥å·¥å…·ç»“æœï¼‰
                
                        node = _agent_graph.ModelRequestNode(
                            request=ModelRequest(parts=[
                                RetryPromptPart(
                                    content=tool_results_text,
                                    timestamp=datetime.now()
                                )
                            ])
                        )
                        
                        
                        # æ›´æ–°ä¸Šä¸€è½®å·¥å…·æ•°é‡
                        react_last_tool_count = current_tool_count
                        
                        # æ¸…ç©ºç´¯ç§¯æ–‡æœ¬å’Œå¤„ç†è®°å½•ï¼Œå‡†å¤‡ä¸‹ä¸€è½®
                        react_accumulated_text = ""
                        react_processed_calls = []
                        
                        logger.info(f"[ReAct] å·²æ³¨å…¥å·¥å…·ç»“æœèŠ‚ç‚¹ï¼Œç»§ç»­ä¸‹ä¸€è½®è¿­ä»£")
                        continue  # ç»§ç»­è¿­ä»£ï¼Œè®© agent åŸºäºå·¥å…·ç»“æœç”Ÿæˆ
            
            # CallToolsNode - å·¥å…·è°ƒç”¨èŠ‚ç‚¹
            elif Agent.is_call_tools_node(node):
                logger.info(node)
                parts = node.model_response.parts
                
                # ğŸ”‘ ReAct æ¨¡å¼ç‰¹æ®Šå¤„ç†ï¼šCallToolsNode å¯èƒ½åªåŒ…å« TextPart
                # åœ¨ ReAct æ¨¡å¼ä¸‹ï¼ŒLLM ä¸ä½¿ç”¨åŸç”Ÿå·¥å…·è°ƒç”¨ï¼Œè€Œæ˜¯è¾“å‡ºæ–‡æœ¬æ ¼å¼çš„ <tool_call>
                if use_react_mode:
                    # æå–æ‰€æœ‰ TextPart
                    text_parts = [p.content for p in parts if p.part_kind == 'text']
                    if text_parts:
                        logger.info(f"[ReAct] CallToolsNode åŒ…å« {len(text_parts)} ä¸ª TextPartï¼Œå¤„ç†å·¥å…·è°ƒç”¨")
                        
                        for text in text_parts:
                            # ä½¿ç”¨ç»Ÿä¸€çš„ process_react_text å‡½æ•°å¤„ç†æ–‡æœ¬
                            async for chunk in process_react_text(
                                text=text,
                                react_accumulated_text=react_accumulated_text,
                                react_processed_calls=react_processed_calls,
                                tool_calls_info=tool_calls_info,
                                deps=deps,
                                react_tools_map=react_tools_map
                            ):
                                # æœ€åä¸€ä¸ªè¿”å›å€¼æ˜¯ tupleï¼ŒåŒ…å«æ›´æ–°åçš„ç´¯ç§¯æ–‡æœ¬
                                if isinstance(chunk, tuple):
                                    react_accumulated_text, _ = chunk
                                else:
                                    yield chunk
                        
                        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ³¨å…¥å·¥å…·ç»“æœèŠ‚ç‚¹ï¼ˆä¸ ModelRequestNode åçš„é€»è¾‘ç›¸åŒï¼‰
                        if react_processed_calls:
                            current_tool_count = len(tool_calls_info)
                            new_tools_count = current_tool_count - react_last_tool_count
                            
                            if new_tools_count > 0:
                                logger.info(f"[ReAct] CallToolsNode ä¸­æ‰§è¡Œäº† {new_tools_count} ä¸ªå·¥å…·ï¼Œæ³¨å…¥ç»“æœèŠ‚ç‚¹")
                                
                                tool_results_text = "\n\n**å·¥å…·æ‰§è¡Œç»“æœ**ï¼š\n\n"
                                for tool_info in tool_calls_info[-new_tools_count:]:
                                    if tool_info.get('success'):
                                        tool_results_text += f"âœ… {tool_info['tool_name']}\n"
                                        tool_results_text += f"```json\n{json.dumps(tool_info['result'], ensure_ascii=False, indent=2)}\n```\n\n"
                                    else:
                                        tool_results_text += f"âŒ {tool_info['tool_name']}: {tool_info.get('error', 'æœªçŸ¥é”™è¯¯')}\n\n"
                                
                                tool_results_text += "è¯·åŸºäºä»¥ä¸Šå·¥å…·è°ƒç”¨ç»“æœç»§ç»­å›ç­”ç”¨æˆ·ã€‚\n"
                                
                       
                                node = _agent_graph.ModelRequestNode(
                                    request=ModelRequest(parts=[
                                        RetryPromptPart(
                                            content=tool_results_text,
                                            timestamp=datetime.now()
                                        )
                                    ])
                                )
                                
                                react_last_tool_count = current_tool_count
                                react_accumulated_text = ""
                                react_processed_calls = []
                                
                                logger.info(f"[ReAct] å·²æ³¨å…¥å·¥å…·ç»“æœèŠ‚ç‚¹ï¼Œç»§ç»­è¿­ä»£")
                                continue
                    
                    # æ²¡æœ‰å·¥å…·è°ƒç”¨æˆ–å·²å¤„ç†å®Œï¼Œç»§ç»­ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
                    continue
                
                # æ ‡å‡†æ¨¡å¼ï¼šæ£€æŸ¥å·¥å…·è°ƒç”¨æ˜¯å¦æ­£ç¡®
                is_valid_tool_call = check_tool_call(parts,is_in_retry_state)
                
                if not is_valid_tool_call:
                    tool_call_retry_count += 1
                    logger.warning(f"âš ï¸ [stream_agent_response] å·¥å…·è°ƒç”¨éªŒè¯å¤±è´¥ï¼ˆé‡è¯• {tool_call_retry_count}/{max_tool_call_retries}ï¼‰")
                    logger.warning(f"   æ¨¡å‹è¾“å‡º: {[p for p in parts if p.part_kind == 'text']}")
                    
                    if tool_call_retry_count < max_tool_call_retries:
                        # æå–æ¨¡å‹è¾“å‡ºçš„æ–‡æœ¬ï¼ˆåŒ…å« <notify>xxx</notify> æ ‡è®°ï¼‰
                        text_parts = [p.content for p in parts if p.part_kind == 'text']
                        model_text = '\n'.join(text_parts) if text_parts else '(æ— æ–‡æœ¬è¾“å‡º)'
                        
                        # æ„é€ é‡è¯•æç¤º
                        retry_message = (
                            f"ä½ è¾“å‡ºäº†å·¥å…·æ ‡è®°<notify></notify>ä½†æ²¡æœ‰å®é™…è°ƒç”¨å·¥å…·ã€‚è¯·æ­£ç¡®ä½¿ç”¨å‡½æ•°è°ƒç”¨åŠŸèƒ½ã€‚\n"
                            f"ä½ çš„è¾“å‡ºï¼š{model_text}\n"
                            f"è¯·é‡æ–°å°è¯•ï¼Œå¿…é¡»è°ƒç”¨å…·ä½“å·¥å…·ï¼è€Œä¸æ˜¯ä»…å£°æ˜<notify>tool_name</notify>ï¼"
                        )
                        
                        logger.info(f"ğŸ”„ [stream_agent_response] æ³¨å…¥é‡è¯•æç¤º: {retry_message[:100]}...")
                        
                        
                        
                        # æ‰‹åŠ¨æ·»åŠ é‡è¯•æ¶ˆæ¯åˆ°èŠ‚ç‚¹
                        # ä½¿ç”¨ RetryPromptPart æ·»åŠ é‡è¯•æç¤ºä½œä¸ºç”¨æˆ·æ¶ˆæ¯
                        
                        node=_agent_graph.ModelRequestNode(request=ModelRequest(parts=[RetryPromptPart(
                                content=retry_message,
                                timestamp=datetime.now()
                            )]))
                        
                        logger.info(f"ğŸ“¨ [stream_agent_response] å·²æ·»åŠ é‡è¯•æ¶ˆæ¯åˆ°ä¸Šä¸‹æ–‡ï¼Œç»§ç»­ä¸‹ä¸€è½®è¿­ä»£")
                        is_in_retry_state = True
                        
                        # é€šçŸ¥å‰ç«¯æ­£åœ¨é‡è¯•
                        yield f"\n\n__RETRY__:{json.dumps({'reason': 'å·¥å…·è°ƒç”¨æ ¼å¼é”™è¯¯', 'retry': tool_call_retry_count, 'max': max_tool_call_retries}, ensure_ascii=False)}\n\n"
                        
                        # è·³è¿‡å½“å‰èŠ‚ç‚¹ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªè¿­ä»£ï¼ˆæ¨¡å‹ä¼šé‡æ–°å°è¯•ï¼‰
                        continue
                    else:
                        # è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒå¹¶æŠ¥é”™
                        error_msg = f"å·¥å…·è°ƒç”¨å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {max_tool_call_retries}ï¼è¯·ä¸­æ­¢æˆ–é‡æ–°ç”Ÿæˆ"
                        logger.error(f"âŒ [stream_agent_response] {error_msg}")
                        yield f"\n\n__ERROR__:{json.dumps({'error': error_msg}, ensure_ascii=False)}\n\n"
                        # ç»§ç»­æ‰§è¡Œï¼Œè®©æ¨¡å‹å°è¯•ç”Ÿæˆæ–‡æœ¬å“åº”
                
                # é‡ç½®é‡è¯•è®¡æ•°å™¨ï¼ˆæˆåŠŸè°ƒç”¨æˆ–è¶…è¿‡é‡è¯•æ¬¡æ•°ï¼‰
                if is_valid_tool_call:
                    tool_call_retry_count = 0
                    is_in_retry_state = False
                    
                    logger.info(f"ğŸ”§ [stream_agent_response] æ£€æµ‹åˆ°æœ‰æ•ˆå·¥å…·è°ƒç”¨èŠ‚ç‚¹, track_tool_calls={track_tool_calls}")
                    if track_tool_calls:
                        async with node.stream(run.ctx) as handle_stream:
                            event_count = 0
                            async for event in handle_stream:
                                event_count += 1
                                logger.info(f"ğŸ” [stream_agent_response] æ”¶åˆ°äº‹ä»¶ #{event_count}, ç±»å‹: {type(event).__name__}")
                                
                                # å·¥å…·è°ƒç”¨äº‹ä»¶
                                if isinstance(event, FunctionToolCallEvent):
                                    logger.info(f"ğŸ“ [stream_agent_response] ç«‹å³æ¨é€å·¥å…·è°ƒç”¨å¼€å§‹: {event.part.tool_name}")
                                    logger.info(f"   å‚æ•°: {event.part.args}")
                                    # ç«‹å³é€šçŸ¥å‰ç«¯å·¥å…·è°ƒç”¨å¼€å§‹
                                    notification = f"\n\n__TOOL_CALL_START__:{json.dumps({'tool_name': event.part.tool_name, 'args': event.part.args}, ensure_ascii=False)}"
                                    yield notification
                                    logger.info(f"âœ… [stream_agent_response] å·²æ¨é€é€šçŸ¥åˆ°æµ")
                                    
                                    tool_calls_info.append({
                                        "tool_name": event.part.tool_name,
                                        "args": event.part.args,
                                        "tool_call_id": event.part.tool_call_id
                                    })
                                # å·¥å…·è¿”å›äº‹ä»¶
                                elif isinstance(event, FunctionToolResultEvent):
                                    logger.info(f"âœ… [stream_agent_response] å·¥å…·æ‰§è¡Œå®Œæˆ: {event.tool_call_id}")
                                    logger.info(f"   è¿”å›ç»“æœ: {event.result.content}")
                                    # æŸ¥æ‰¾å¯¹åº”çš„å·¥å…·è°ƒç”¨å¹¶æ·»åŠ è¿”å›ç»“æœ
                                    for call_info in tool_calls_info:
                                        if call_info.get("tool_call_id") == event.tool_call_id:
                                            call_info["result"] = event.result.content
                                            logger.info(f"ğŸ“ [stream_agent_response] å·²è®°å½•å·¥å…·ç»“æœ")
                                            break
                            
                            logger.info(f"ğŸ [stream_agent_response] å·¥å…·è°ƒç”¨èŠ‚ç‚¹å¤„ç†å®Œæˆï¼Œå…± {event_count} ä¸ªäº‹ä»¶")
                            
                            # å·¥å…·è°ƒç”¨å®Œæˆåï¼Œåœ¨åç»­æ–‡æœ¬å‰åŠ ä¸Šæ¢è¡Œï¼Œé¿å…ç´§æŒ¨åœ¨ä¸€èµ·
                            if event_count > 0:
                                yield "\n\n"
                    else:
                        logger.warning(f"âš ï¸ [stream_agent_response] track_tool_calls=Falseï¼Œè·³è¿‡å·¥å…·è°ƒç”¨è¿½è¸ª")
            
            # End - ç»“æŸèŠ‚ç‚¹
            elif Agent.is_end_node(node):
                logger.info(f"ğŸ¬ [stream_agent_response] åˆ°è¾¾ç»“æŸèŠ‚ç‚¹")
                # è¿è¡Œå®Œæˆï¼Œç«‹å³é€€å‡ºå¾ªç¯
                break
    
    # æµç»“æŸåï¼Œè¿”å›å·¥å…·è°ƒç”¨æ‘˜è¦ï¼ˆä»…æ ‡å‡†æ¨¡å¼ï¼‰
    # ReAct æ¨¡å¼å·²ç»é€šè¿‡ __TOOL_EXECUTED__ é€ä¸ªé€šçŸ¥å‰ç«¯ï¼Œæ— éœ€å†å‘é€æ‘˜è¦
    if track_tool_calls and tool_calls_info and not use_react_mode:
        yield f"\n\n__TOOL_SUMMARY__:{json.dumps({'type': 'tools_executed', 'tools': tool_calls_info}, ensure_ascii=False)}"

async def run_llm_agent(
    session: Session,
    llm_config_id: int,
    user_prompt: str,
    output_type: Type[BaseModel],
    system_prompt: Optional[str] = None,
    deps: str = "",
    max_tokens: Optional[int] = None,
    max_retries: int = 3,
    temperature: Optional[float] = None,
    timeout: Optional[float] = None,
    track_stats: bool = True,) -> BaseModel:
    """
    è¿è¡ŒLLM Agentçš„æ ¸å¿ƒå°è£…ã€‚
    æ”¯æŒæ¸©åº¦/æœ€å¤§tokens/è¶…æ—¶ï¼ˆé€šè¿‡ ModelSettings æ³¨å…¥ï¼‰ã€‚
    """
    # é™é¢é¢„æ£€ï¼ˆæŒ‰ä¼°ç®—çš„è¾“å…¥ tokens + 1 æ¬¡è°ƒç”¨ï¼‰
    if track_stats:
        ok, reason = _precheck_quota(session, llm_config_id, _calc_input_tokens(system_prompt, user_prompt), need_calls=1)
        if not ok:
            raise ValueError(f"LLM é…é¢ä¸è¶³:{reason}")

    agent = _get_agent(
        session,
        llm_config_id,
        output_type,
        system_prompt or '',
        temperature=temperature,
        max_tokens=max_tokens,
        timeout=timeout,
    )
    
    logger.info(f"system_prompt: {system_prompt}")
    logger.info(f"user_prompt: {user_prompt}")
    
    last_exception = None
    for attempt in range(max_retries):
        try:
            response=await agent.run(user_prompt, deps=deps) #await run_agent_with_streaming(agent, user_prompt, deps=deps)
            response=response.output
            logger.info(f"response: {response}")
            # ç»Ÿè®¡ï¼šè¾“å…¥/è¾“å‡º tokens ä¸è°ƒç”¨æ¬¡æ•°
            if track_stats:
                in_tokens = _calc_input_tokens(system_prompt, user_prompt)
                try:
                    out_text = response if isinstance(response, str) else json.dumps(response, ensure_ascii=False)
                except Exception:
                    out_text = str(response)
                out_tokens = _estimate_tokens(out_text)
                _record_usage(session, llm_config_id, in_tokens, out_tokens, calls=1, aborted=False)
            return response
        except asyncio.CancelledError:
            logger.info("LLM è°ƒç”¨è¢«å–æ¶ˆï¼ˆCancelledErrorï¼‰ï¼Œç«‹å³ä¸­æ­¢ï¼Œä¸å†é‡è¯•ã€‚")
            if track_stats:
                in_tokens = _calc_input_tokens(system_prompt, user_prompt)
                _record_usage(session, llm_config_id, in_tokens, 0, calls=1, aborted=True)
            raise
        except Exception as e:
            last_exception = e
            logger.warning(f"Agent execution failed on attempt {attempt + 1}/{max_retries} for llm_config_id {llm_config_id}: {e}")

    logger.error(f"Agent execution failed after {max_retries} attempts for llm_config_id {llm_config_id}. Last error: {last_exception}")
    raise ValueError(f"è°ƒç”¨LLMæœåŠ¡å¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡: {str(last_exception)}")



from app.services.assistant_tools.pydantic_ai_tools import AssistantDeps, get_tools_schema, ASSISTANT_TOOLS

async def generate_assistant_chat_streaming(
    session: Session,
    request: AssistantChatRequest,
    system_prompt: str,
    tools: list,  #  ç›´æ¥æ¥å—å·¥å…·å‡½æ•°åˆ—è¡¨
    deps,  #  ä¾èµ–ä¸Šä¸‹æ–‡ï¼ˆAssistantDeps å®ä¾‹ï¼‰
    track_stats: bool = True) -> AsyncGenerator[str, None]:
    """
    çµæ„ŸåŠ©æ‰‹ä¸“ç”¨æµå¼å¯¹è¯ç”Ÿæˆã€‚
    
    å‚æ•°ï¼š
    - request: AssistantChatRequestï¼ˆåŒ…å«å¯¹è¯å†å²ã€å¡ç‰‡ä¸Šä¸‹æ–‡ç­‰ï¼‰
    - system_prompt: ç³»ç»Ÿæç¤ºè¯
    - tools: å·¥å…·å‡½æ•°åˆ—è¡¨ï¼ˆç›´æ¥ä¼ å‡½æ•°ï¼Œç¬¦åˆ Pydantic AI æ ‡å‡†ç”¨æ³•ï¼‰
    - deps: å·¥å…·ä¾èµ–ä¸Šä¸‹æ–‡ï¼ˆAssistantDeps å®ä¾‹ï¼‰
    - track_stats: æ˜¯å¦ç»Ÿè®¡ token ä½¿ç”¨
    """
    
    # ç›´æ¥ä½¿ç”¨å‰ç«¯å‘é€çš„ä¸Šä¸‹æ–‡å’Œç”¨æˆ·è¾“å…¥
    parts = []
    
    # 1. é¡¹ç›®ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆåŒ…å«ç»“æ„æ ‘ã€ç»Ÿè®¡ã€æ“ä½œå†å²ç­‰ï¼‰
    if request.context_info:
        parts.append(request.context_info)
    
    # 2. ç”¨æˆ·å½“å‰è¾“å…¥
    if request.user_prompt:
        parts.append(f"\nUser: {request.user_prompt}")
        
        # 3. åœ¨ç”¨æˆ·è¾“å…¥åç«‹å³æ·»åŠ å·¥å…·è°ƒç”¨å¼ºåŒ–æç¤ºï¼ˆç´§é‚»æ¨¡å‹è¾“å‡ºä½ç½®ï¼‰
        tool_reminder = """

---
**ã€âš ï¸ æ¥è‡ªç³»ç»Ÿçš„å…³é”®æé†’ã€‘**

**é¡¹ç›®ç»“æ„åŸºå‡†åŸåˆ™**ï¼š
ä½ å¿…é¡»ä»¥**å½“å‰æç¤ºè¯ä¸­çš„é¡¹ç›®ç»“æ„æ ‘**ä¸ºå‡†ï¼Œå¿½ç•¥å†å²å¯¹è¯ä¸­çš„ä»»ä½•è¿‡æ—¶ä¿¡æ¯ï¼
- é¡¹ç›®ç»“æ„æ ‘ä¼šå®æ—¶æ›´æ–°ï¼ˆç”¨æˆ·å¯èƒ½ç§»åŠ¨ã€é‡ç»„å¡ç‰‡ï¼‰
- å¦‚æœç”¨æˆ·è¯¢é—®å¡ç‰‡ä½ç½®æˆ–å±‚çº§å…³ç³»ï¼Œä»¥**æœ€æ–°çš„æ ‘å½¢ç»“æ„**ä¸ºå‡†
- å†å²å¯¹è¯ä¸­çš„ç»“æ„ä¿¡æ¯å¯èƒ½å·²è¿‡æ—¶ï¼Œä¸è¦ä¾èµ–å®ƒ
- è¿‘æœŸæ“ä½œè®°å½•ä¼šæ˜¾ç¤ºæœ€æ–°çš„ç§»åŠ¨/å˜æ›´ä¿¡æ¯

**å¡ç‰‡åˆ›ä½œè§„åˆ™**ï¼š
åœ¨åˆ›å»ºå¡ç‰‡æˆ–è®¨è®ºå¡ç‰‡æ–¹æ¡ˆä¹‹å‰ï¼Œå¿…é¡»ç¡®ä¿å·²çŸ¥è¯¥ç±»å‹å¡ç‰‡çš„ Schema ç»“æ„ï¼
- å¦‚æœä¸ç¡®å®šå­—æ®µï¼Œå…ˆè°ƒç”¨ get_card_type_schema è·å–ç»“æ„
- ä¸è¦å‡­æƒ³è±¡çŒœæµ‹å­—æ®µåï¼Œå¿…é¡»ç²¾ç¡®åŒ¹é… Schema

**å·¥å…·è°ƒç”¨æ­¥éª¤**ï¼š
å¦‚æœéœ€è¦æ‰§è¡Œæ“ä½œï¼ˆæŸ¥è¯¢ã€åˆ›å»ºã€ä¿®æ”¹å¡ç‰‡ç­‰ï¼‰ï¼Œä½ å¿…é¡»ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ­¥éª¤æ‰§è¡Œï¼š
1. å…ˆè¾“å‡º `<notify>å·¥å…·å</notify>` æ ‡è®°ï¼ˆå¦‚ `<notify>create_card</notify>`ï¼‰
2. ç«‹å³è°ƒç”¨å¯¹åº”çš„å‡½æ•°å·¥å…·ï¼ç‰¹åˆ«æ³¨æ„ï¼Œ<notify>tool_name</notify>ä»…ä»…æ˜¯å£°æ˜ä½ è¦è°ƒç”¨å·¥å…·ï¼Œå¹¶ä¸ä¼šè§¦å‘å®é™…çš„å·¥å…·è°ƒç”¨ï¼Œä½ è¿˜éœ€è¦è¿›è¡Œå®é™…è°ƒç”¨ï¼
3. ç­‰å¾…å·¥å…·è¿”å› `{"success": true, ...}` åå†å‘ç”¨æˆ·ç¡®è®¤

âŒ ä¸¥ç¦ï¼šåªæè¿°è¦è°ƒç”¨ä»€ä¹ˆå·¥å…·ï¼Œå´ä¸å®é™…è°ƒç”¨å‡½æ•°
âœ… æ­£ç¡®ï¼šè¾“å‡º <notify> æ ‡è®° â†’ è°ƒç”¨å‡½æ•° â†’ ç¡®è®¤ç»“æœ

è¯·ç«‹å³æŒ‰æ­¤æµç¨‹å¤„ç†ç”¨æˆ·è¯·æ±‚ã€‚
---
"""
        parts.append(tool_reminder)
    
    final_user_prompt = "\n\n".join(parts) if parts else "ï¼ˆç”¨æˆ·æœªè¾“å…¥æ–‡å­—ï¼Œå¯èƒ½æ˜¯æƒ³æŸ¥çœ‹é¡¹ç›®ä¿¡æ¯æˆ–éœ€è¦å¸®åŠ©ï¼‰"
    
    logger.info(f"çµæ„ŸåŠ©æ‰‹ system_prompt: {system_prompt}...")
    logger.info(f"çµæ„ŸåŠ©æ‰‹ final_user_prompt: {final_user_prompt}...")
    
    # é™é¢é¢„æ£€
    if track_stats:
        ok, reason = _precheck_quota(session, request.llm_config_id, _calc_input_tokens(system_prompt, final_user_prompt), need_calls=1)
        if not ok:
            raise ValueError(f"LLM é…é¢ä¸è¶³:{reason}")
    
    
    # ç›´æ¥åœ¨åˆ›å»ºæ—¶ä¼ å…¥å·¥å…·åˆ—è¡¨
    agent = _get_agent(
        session=session,
        llm_config_id=request.llm_config_id,
  
        system_prompt=system_prompt,
        temperature=request.temperature or 0.6,
        max_tokens=request.max_tokens or 8192,
        timeout=request.timeout or 60,
        deps_type=AssistantDeps,
        tools=tools  # ç›´æ¥ä¼ å…¥å·¥å…·å‡½æ•°åˆ—è¡¨
    )
    
    # æµå¼ç”Ÿæˆå“åº”
    accumulated = ""
    
    try:
        async for chunk in stream_agent_response(
            agent,
            final_user_prompt,
            deps=deps,  # ä¼ å…¥ä¾èµ–ä¸Šä¸‹æ–‡
            track_tool_calls=True
        ):
            accumulated += chunk
            yield chunk
        
    except asyncio.CancelledError:
        if track_stats:
            in_tokens = _calc_input_tokens(system_prompt, final_user_prompt)
            out_tokens = _estimate_tokens(accumulated)
            _record_usage(session, request.llm_config_id, in_tokens, out_tokens, calls=1, aborted=True)
        return
    except Exception as e:
        logger.error(f"çµæ„ŸåŠ©æ‰‹ç”Ÿæˆå¤±è´¥: {e}")
        # å³ä½¿å¤±è´¥ä¹Ÿè¦å‘é€é”™è¯¯æ‘˜è¦ï¼Œè®©å‰ç«¯æ¸…é™¤"æ­£åœ¨è°ƒç”¨å·¥å…·"çŠ¶æ€
        yield f"\n\n__ERROR__:{json.dumps({'type': 'error', 'message': str(e)}, ensure_ascii=False)}"
        raise
    
    # ç»Ÿè®¡
    if track_stats:
        try:
            in_tokens = _calc_input_tokens(system_prompt, final_user_prompt)
            out_tokens = _estimate_tokens(accumulated)
            _record_usage(session, request.llm_config_id, in_tokens, out_tokens, calls=1, aborted=False)
        except Exception as stat_e:
            logger.warning(f"è®°å½•çµæ„ŸåŠ©æ‰‹ç»Ÿè®¡å¤±è´¥: {stat_e}")


async def generate_assistant_chat_streaming_react(
    session: Session,
    request: AssistantChatRequest,
    system_prompt: str,
    track_stats: bool = True
) -> AsyncGenerator[str, None]:
    """
    çµæ„ŸåŠ©æ‰‹ ReAct æ¨¡å¼æµå¼å¯¹è¯ç”Ÿæˆã€‚
    
    ä¸æ ‡å‡†æ¨¡å¼çš„åŒºåˆ«ï¼š
    - ä¸ä½¿ç”¨åŸç”Ÿ Function Callingï¼Œè€Œæ˜¯è®©æ¨¡å‹ä»¥æ–‡æœ¬æ ¼å¼è¾“å‡ºå·¥å…·è°ƒç”¨
    - ç³»ç»Ÿè´Ÿè´£è§£æ <tool_call> æ ‡è®°å¹¶æ‰§è¡Œå·¥å…·
    - å…¼å®¹æ›´å¤šä¸æ”¯æŒ Function Calling çš„æ¨¡å‹
    
    å·¥å…·è°ƒç”¨æ ¼å¼ï¼š
    ```
    <tool_call>
    {
      "name": "tool_name",
      "args": {...}
    }
    </tool_call>
    ```
    
    âš ï¸ é‡è¦æ³¨æ„äº‹é¡¹ï¼š
    1. **å¡ç‰‡åˆ›ä½œè§„åˆ™**ï¼šåœ¨åˆ›å»ºå¡ç‰‡æˆ–è®¨è®ºå¡ç‰‡æ–¹æ¡ˆæ—¶ï¼ŒLLM å¿…é¡»å…ˆè°ƒç”¨ get_card_type_schema 
       è·å–è¯¥ç±»å‹å¡ç‰‡çš„ Schema ç»“æ„ï¼Œä¸èƒ½å‡­æƒ³è±¡çŒœæµ‹å­—æ®µ
    2. **JSON æ ¼å¼**ï¼šä½¿ç”¨ json_repair è‡ªåŠ¨ä¿®å¤å¸¸è§é”™è¯¯ï¼Œä½†ä»å»ºè®®æç¤ºè¯ä¸­å¼ºè°ƒæ­£ç¡®æ ¼å¼
    3. **å·¥å…·ç»“æœåé¦ˆ**ï¼šå·¥å…·æ‰§è¡Œç»“æœä¼šé€šè¿‡ __TOOL_EXECUTED__ åè®®æ ‡è®°å‘é€ç»™å‰ç«¯
    4. **å¯¹è¯å†å²**ï¼šå·¥å…·è°ƒç”¨è®°å½•ä¼šè¢«å‰ç«¯æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­ï¼Œä¾›åç»­å¯¹è¯å‚è€ƒ
    """
    from app.services.assistant_tools.pydantic_ai_tools import (
        search_cards, create_card, modify_card_field, replace_field_text,
        batch_create_cards, get_card_type_schema, get_card_content
    )
    
    # å·¥å…·æ˜ å°„è¡¨ï¼ˆæ‰‹åŠ¨æ‰§è¡Œï¼‰
    TOOL_FUNCTIONS = {
        "search_cards": search_cards,
        "create_card": create_card,
        "modify_card_field": modify_card_field,
        "replace_field_text": replace_field_text,
        "batch_create_cards": batch_create_cards,
        "get_card_type_schema": get_card_type_schema,
        "get_card_content": get_card_content,
    }
    
    # è·å–å·¥å…· schema
    tools_schema = await get_tools_schema()
    tools_schema_text = json.dumps(tools_schema, ensure_ascii=False, indent=2)
    
    # åœ¨ç³»ç»Ÿæç¤ºä¸­æ³¨å…¥å·¥å…·å®šä¹‰
    enhanced_system_prompt = system_prompt.replace("{tools_schema}", tools_schema_text)
    
    # ç»„è£…ç”¨æˆ·æç¤ºï¼ˆä¸æ ‡å‡†æ¨¡å¼ç›¸åŒï¼‰
    parts = []
    if request.context_info:
        parts.append(request.context_info)
    
    if request.user_prompt:
        parts.append(f"\nUser: {request.user_prompt}")
        
        # React æ¨¡å¼çš„å·¥å…·è°ƒç”¨æé†’
        tool_reminder = """

---
**ã€âš ï¸ ReAct æ¨¡å¼å…³é”®æé†’ã€‘**

**0. é¡¹ç›®ç»“æ„åŸºå‡†åŸåˆ™**ï¼š
ä½ å¿…é¡»ä»¥**å½“å‰æç¤ºè¯ä¸­çš„é¡¹ç›®ç»“æ„æ ‘**ä¸ºå‡†ï¼Œå¿½ç•¥å†å²å¯¹è¯ä¸­çš„ä»»ä½•è¿‡æ—¶ä¿¡æ¯ï¼
- é¡¹ç›®ç»“æ„æ ‘ä¼šå®æ—¶æ›´æ–°ï¼ˆç”¨æˆ·å¯èƒ½ç§»åŠ¨ã€é‡ç»„å¡ç‰‡ï¼‰
- å¦‚æœç”¨æˆ·è¯¢é—®å¡ç‰‡ä½ç½®æˆ–å±‚çº§å…³ç³»ï¼Œä»¥**æœ€æ–°çš„æ ‘å½¢ç»“æ„**ä¸ºå‡†
- å†å²å¯¹è¯ä¸­çš„ç»“æ„ä¿¡æ¯å¯èƒ½å·²è¿‡æ—¶ï¼Œä¸è¦ä¾èµ–å®ƒ
- è¿‘æœŸæ“ä½œè®°å½•ä¼šæ˜¾ç¤ºæœ€æ–°çš„ç§»åŠ¨/å˜æ›´ä¿¡æ¯

**1. å¡ç‰‡åˆ›ä½œè§„åˆ™**ï¼š
åœ¨åˆ›å»ºå¡ç‰‡æˆ–è®¨è®ºå¡ç‰‡æ–¹æ¡ˆä¹‹å‰ï¼Œå¿…é¡»ç¡®ä¿å·²çŸ¥è¯¥ç±»å‹å¡ç‰‡çš„ Schema ç»“æ„ï¼
- å¦‚æœä¸ç¡®å®šå­—æ®µï¼Œå…ˆè°ƒç”¨ get_card_type_schema è·å–ç»“æ„
- ä¸è¦å‡­æƒ³è±¡çŒœæµ‹å­—æ®µåï¼Œå¿…é¡»ç²¾ç¡®åŒ¹é… Schema
- æ¯ç§å¡ç‰‡ç±»å‹çš„å­—æ®µéƒ½ä¸åŒ

**2. å·¥å…·è°ƒç”¨æ ¼å¼**ï¼š
<tool_call>
{
  "name": "å·¥å…·åç§°",
  "args": { "å‚æ•°å": "å‚æ•°å€¼" }
}
</tool_call>

**3. JSON æ ¼å¼è¦æ±‚**ï¼š
- æ‰€æœ‰å­—ç¬¦ä¸²å¿…é¡»æ­£ç¡®é—­åˆï¼ˆæ¯ä¸ª " éƒ½è¦æœ‰é—­åˆçš„ "ï¼‰
- æ•°ç»„å¿…é¡»æ­£ç¡®é—­åˆï¼ˆæ¯ä¸ª [ éƒ½è¦æœ‰é—­åˆçš„ ]ï¼‰
- å­—ç¬¦ä¸²å†…æ¢è¡Œä½¿ç”¨ \nï¼Œä¸è¦ä½¿ç”¨çœŸå®æ¢è¡Œ
- ç¡®ä¿æ‰€æœ‰æ‹¬å·ã€å¼•å·éƒ½æˆå¯¹å‡ºç°

âŒ é”™è¯¯ï¼š"description": "æ–‡æœ¬ï¼Œ
âœ… æ­£ç¡®ï¼š"description": "æ–‡æœ¬"
---
"""
        parts.append(tool_reminder)
    
    final_user_prompt = "\n\n".join(parts) if parts else "ï¼ˆç”¨æˆ·æœªè¾“å…¥æ–‡å­—ï¼Œå¯èƒ½æ˜¯æƒ³æŸ¥çœ‹é¡¹ç›®ä¿¡æ¯æˆ–éœ€è¦å¸®åŠ©ï¼‰"
    
    logger.info(f"[ReAct] system_prompt é•¿åº¦: {len(enhanced_system_prompt)}")
    logger.info(f"[ReAct] final_user_prompt: {final_user_prompt}...")
    
    # é™é¢é¢„æ£€
    if track_stats:
        ok, reason = _precheck_quota(session, request.llm_config_id, _calc_input_tokens(enhanced_system_prompt, final_user_prompt), need_calls=1)
        if not ok:
            raise ValueError(f"LLM é…é¢ä¸è¶³:{reason}")
    
    # åˆ›å»ºä¸å¸¦å·¥å…·ç»‘å®šçš„ Agent
    agent = _get_agent(
        session=session,
        llm_config_id=request.llm_config_id,
        system_prompt=enhanced_system_prompt,
        temperature=request.temperature or 0.6,
        max_tokens=request.max_tokens or 8192,
        timeout=request.timeout or 60,
        deps_type=AssistantDeps,
        tools=[]  # ReAct æ¨¡å¼ä¸ç»‘å®šå·¥å…·
    )
    
    # åˆ›å»ºä¾èµ–ä¸Šä¸‹æ–‡
    deps = AssistantDeps(session=session, project_id=request.project_id)
    
    # ä½¿ç”¨ç»Ÿä¸€çš„ stream_agent_responseï¼Œä¼ å…¥ ReAct å‚æ•°
    accumulated = ""
    
    try:
        async for chunk in stream_agent_response(
            agent=agent,
            user_prompt=final_user_prompt,
            deps=deps,
            message_history=None,
            track_tool_calls=True,
            use_react_mode=True,
            react_tools_map=TOOL_FUNCTIONS
        ):
            accumulated += chunk
            yield chunk
    
    except asyncio.CancelledError:
        if track_stats:
            in_tokens = _calc_input_tokens(enhanced_system_prompt, final_user_prompt)
            out_tokens = _estimate_tokens(accumulated)
            _record_usage(session, request.llm_config_id, in_tokens, out_tokens, calls=1, aborted=True)
        return
    except Exception as e:
        logger.error(f"[ReAct] ç”Ÿæˆå¤±è´¥: {e}")
        yield f"\n\n__ERROR__:{json.dumps({'type': 'error', 'message': str(e)}, ensure_ascii=False)}"
        raise
    
    # ç»Ÿè®¡
    if track_stats:
        try:
            in_tokens = _calc_input_tokens(enhanced_system_prompt, final_user_prompt)
            out_tokens = _estimate_tokens(accumulated)
            _record_usage(session, request.llm_config_id, in_tokens, out_tokens, calls=1, aborted=False)
        except Exception as stat_e:
            logger.warning(f"[ReAct] è®°å½•ç»Ÿè®¡å¤±è´¥: {stat_e}")


async def generate_continuation_streaming(session: Session, request: ContinuationRequest, system_prompt: str, track_stats: bool = True) -> AsyncGenerator[str, None]:
    """ä»¥æµå¼æ–¹å¼ç”Ÿæˆç»­å†™å†…å®¹ã€‚system_prompt ç”±å¤–éƒ¨æ˜¾å¼ä¼ å…¥ã€‚"""
    # ç»„è£…ç”¨æˆ·æ¶ˆæ¯
    user_prompt_parts = []
    
    # 1. æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¼•ç”¨ä¸Šä¸‹æ–‡ + äº‹å®å­å›¾ï¼‰
    context_info = (getattr(request, 'context_info', None) or '').strip()
    if context_info:
        # æ£€æµ‹ context_info æ˜¯å¦å·²åŒ…å«ç»“æ„åŒ–æ ‡è®°ï¼ˆå¦‚ã€å¼•ç”¨ä¸Šä¸‹æ–‡ã€‘ã€ã€ä¸Šæ–‡ã€‘ç­‰ï¼‰
        has_structured_marks = any(mark in context_info for mark in ['ã€å¼•ç”¨ä¸Šä¸‹æ–‡ã€‘', 'ã€ä¸Šæ–‡ã€‘', 'ã€éœ€è¦æ¶¦è‰²', 'ã€éœ€è¦æ‰©å†™'])
        
        if has_structured_marks:
            # å·²ç»æ˜¯ç»“æ„åŒ–çš„ä¸Šä¸‹æ–‡ï¼Œç›´æ¥ä½¿ç”¨ï¼Œä¸å†é¢å¤–åŒ…è£¹
            user_prompt_parts.append(context_info)
        else:
            # æœªç»“æ„åŒ–çš„ä¸Šä¸‹æ–‡ï¼ˆè€æ ¼å¼ï¼‰ï¼Œæ·»åŠ æ ‡è®°
            user_prompt_parts.append(f"ã€å‚è€ƒä¸Šä¸‹æ–‡ã€‘\n{context_info}")
    
    # 2. æ·»åŠ å·²æœ‰ç« èŠ‚å†…å®¹ï¼ˆä»…å½“ previous_content éç©ºæ—¶ï¼‰
    previous_content = (request.previous_content or '').strip()
    if previous_content:
        user_prompt_parts.append(f"ã€å·²æœ‰ç« èŠ‚å†…å®¹ã€‘\n{previous_content}")
        
        # æ·»åŠ å­—æ•°ç»Ÿè®¡ä¿¡æ¯
        existing_word_count = getattr(request, 'existing_word_count', None)
        if existing_word_count is not None:
            user_prompt_parts.append(f"ï¼ˆå·²æœ‰å†…å®¹å­—æ•°ï¼š{existing_word_count} å­—ï¼‰")
        
        # ç»­å†™æŒ‡ä»¤
        if getattr(request, 'append_continuous_novel_directive', True):
            user_prompt_parts.append("ã€æŒ‡ä»¤ã€‘è¯·æ¥ç€ä¸Šè¿°å†…å®¹ç»§ç»­å†™ä½œï¼Œä¿æŒæ–‡é£å’Œå‰§æƒ…è¿è´¯ã€‚ç›´æ¥è¾“å‡ºå°è¯´æ­£æ–‡ã€‚")
    else:
        # æ–°å†™æ¨¡å¼æˆ–æ¶¦è‰²/æ‰©å†™æ¨¡å¼ï¼ˆprevious_content ä¸ºç©ºï¼‰
        # åªåœ¨éœ€è¦æ—¶æ·»åŠ æŒ‡ä»¤
        if getattr(request, 'append_continuous_novel_directive', True):
            # å¦‚æœ context_info ä¸­æœ‰ç»­å†™ç›¸å…³æ ‡è®°ï¼Œè¯´æ˜æ˜¯ç»­å†™åœºæ™¯
            if context_info and 'ã€å·²æœ‰ç« èŠ‚å†…å®¹ã€‘' in context_info:
                user_prompt_parts.append("ã€æŒ‡ä»¤ã€‘è¯·æ¥ç€ä¸Šè¿°å†…å®¹ç»§ç»­å†™ä½œï¼Œä¿æŒæ–‡é£å’Œå‰§æƒ…è¿è´¯ã€‚ç›´æ¥è¾“å‡ºå°è¯´æ­£æ–‡ã€‚")
            else:
                user_prompt_parts.append("ã€æŒ‡ä»¤ã€‘è¯·å¼€å§‹åˆ›ä½œæ–°ç« èŠ‚ã€‚ç›´æ¥è¾“å‡ºå°è¯´æ­£æ–‡ã€‚")
    
    user_prompt = "\n\n".join(user_prompt_parts)
    
    # é™é¢é¢„æ£€
    if track_stats:
        ok, reason = _precheck_quota(session, request.llm_config_id, _calc_input_tokens(system_prompt, user_prompt), need_calls=1)
        if not ok:
            raise ValueError(f"LLM é…é¢ä¸è¶³:{reason}")

    agent = _get_agent(
        session,
        request.llm_config_id,
        output_type=BaseModel,
        system_prompt=system_prompt,
        temperature=request.temperature,
        max_tokens=request.max_tokens,
        timeout=request.timeout,
    )

    try:
        logger.debug(f"æ­£åœ¨ä»¥æµå¼æ¨¡å¼è¿è¡Œ agent")
        async with agent.run_stream(user_prompt) as result:
            accumulated: str = ""
            # ç»Ÿè®¡ç”¨ï¼šç´¯ç§¯è¾“å‡ºå­—ç¬¦æ•°
            out_chars: int = 0
            async for text_chunk in result.stream():
                try:
                    chunk = str(text_chunk)
                except Exception:
                    chunk = ""
                if not chunk:
                    continue
                # è‹¥è¿”å›çš„æ˜¯ç´¯è®¡æ–‡æœ¬ï¼Œåªå‘é€æ–°å¢å·®é‡
                if accumulated and chunk.startswith(accumulated):
                    delta = chunk[len(accumulated):]
                else:
                    # å›é€€ï¼šæ— æ³•åˆ¤æ–­å‰ç¼€æ—¶ï¼Œç›´æ¥æŠŠæœ¬æ¬¡ chunk ä½œä¸ºå¢é‡
                    delta = chunk
                if delta:
                    out_chars += len(delta)
                    yield delta
                if len(chunk) > len(accumulated):
                    accumulated = chunk
    except asyncio.CancelledError:
        logger.info("æµå¼ LLM è°ƒç”¨è¢«å–æ¶ˆï¼ˆCancelledErrorï¼‰ï¼Œåœæ­¢æ¨é€ã€‚")
        if track_stats:
            in_tokens = _calc_input_tokens(system_prompt, user_prompt)
            out_tokens = _estimate_tokens(accumulated)
            _record_usage(session, request.llm_config_id, in_tokens, out_tokens, calls=1, aborted=True)
        return
    except Exception as e:
        logger.error(f"æµå¼ LLM è°ƒç”¨å¤±è´¥: {e}")
        raise
    # æ­£å¸¸ç»“æŸåç»Ÿè®¡
    try:
        if track_stats:
            in_tokens = _calc_input_tokens(system_prompt, user_prompt)
            out_tokens = _estimate_tokens(accumulated)
            _record_usage(session, request.llm_config_id, in_tokens, out_tokens, calls=1, aborted=False)
    except Exception as stat_e:
        logger.warning(f"è®°å½• LLM æµå¼ç»Ÿè®¡å¤±è´¥: {stat_e}")


def create_validator(model_type: Type[BaseModel]) -> Callable[[Any, Any], Awaitable[BaseModel]]:
    '''åˆ›å»ºé€šç”¨çš„ç»“æœéªŒè¯å™¨'''

    async def validate_result(
        ctx: Any,
        result: Response,
    ) -> Response:
        """
        é€šç”¨ç»“æœéªŒè¯å‡½æ•°
        """
        try:
            if model_type is BaseModel or model_type is str: 
                return result
        except Exception:
            return result

        # å°è¯•è§£æä¸ºç›®æ ‡æ¨¡å‹
        parsed: BaseModel
        if isinstance(result, model_type):
            parsed = result
        else:
            try:
                print(f"result: {result}")
                parsed = model_type.model_validate_json(repair_json(result))
            except ValidationError as e:
                err_msg = e.json(include_url=False)
                print(f"Invalid {err_msg}\nè¯·ä¸¥æ ¼æŒ‰ç…§OutputFormatæ ¼å¼è¿”å›ï¼Œç¦æ­¢è¯¢é—®ç»†èŠ‚ï¼Œè‡ªè¡Œåˆ›ä½œ/æ¨æ–­ä¸ç¡®å®šä¿¡æ¯ï¼Œä¸è¦è¿”å›ä»»ä½•å¤šä½™çš„ä¿¡æ¯ï¼")
                raise ModelRetry(f"Invalid  {err_msg}\nè¯·ä¸¥æ ¼æŒ‰ç…§OutputFormatæ ¼å¼è¿”å›ï¼Œç¦æ­¢è¯¢é—®ç»†èŠ‚ï¼Œè‡ªè¡Œåˆ›ä½œ/æ¨æ–­ä¸ç¡®å®šä¿¡æ¯ï¼Œä¸è¦è¿”å›ä»»ä½•å¤šä½™çš„ä¿¡æ¯ï¼")
            except Exception as e:
                print("Exception:", e)
                raise ModelRetry(f'Invalid {e}\nè¯·ä¸¥æ ¼æŒ‰ç…§OutputFormatæ ¼å¼è¿”å›ï¼Œç¦æ­¢è¯¢é—®ç»†èŠ‚ï¼Œè‡ªè¡Œåˆ›ä½œ/æ¨æ–­ä¸ç¡®å®šä¿¡æ¯ï¼Œä¸è¦è¿”å›ä»»ä½•å¤šä½™çš„ä¿¡æ¯ï¼') from e

        # === é’ˆå¯¹ StageLine/ChapterOutline/Chapter çš„å®ä½“å­˜åœ¨æ€§æ ¡éªŒ ===
        try:
            # è§£æ deps ä¸­çš„å®ä½“åç§°é›†åˆ
            all_names: set[str] = set()
            try:
                deps_obj = json.loads(getattr(ctx, 'deps', '') or '{}')
                for nm in (deps_obj.get('all_entity_names') or []):
                    if isinstance(nm, str) and nm.strip():
                        all_names.add(nm.strip())
            except Exception:
                all_names = set()
                
            

            def _check_entity_list(obj: Any) -> list[str]:
                bad: list[str] = []
                try:
                    items = getattr(obj, 'entity_list', None)
                    if isinstance(items, list):
                        for it in items:
                            nm = getattr(it, 'name', None)
                            if isinstance(nm, str) and nm.strip():
                                if all_names and nm.strip() not in all_names:
                                    bad.append(nm.strip())
                except Exception:
                    pass
                return bad

            invalid: list[str] = []
            if isinstance(parsed, (StageLine, ChapterOutline, Chapter)):
                logger.info(f"å¼€å§‹æ ¡éªŒå®ä½“,all_names: {all_names}")
                invalid = _check_entity_list(parsed)
       

            if invalid:
                raise ModelRetry(f"å®ä½“ä¸å­˜åœ¨: {', '.join(sorted(set(invalid)))}ï¼Œè¯·ä»…ä»æä¾›çš„å®ä½“åˆ—è¡¨ä¸­é€‰æ‹©")
        except ModelRetry:
            raise
        except Exception:
            # æ ¡éªŒè¿‡ç¨‹ä¸­ä¸åº”é˜»å¡ä¸»æµç¨‹ï¼Œå¦‚è§£æå¤±è´¥åˆ™å¿½ç•¥
            pass

        return parsed 

    return validate_result